{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e04d1efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import functools\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from linguistic_correcter import LinguisticCorrection\n",
    "from shortcuts_dictionary import ShortcutsDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "763dbb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "from index_query import IndexQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01a724f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from linguistic_correcter import LinguisticCorrection\n",
    "from shortcuts_dictionary import ShortcutsDictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c74b12e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a45eaad2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (165605757.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    self.corpusDir = args.get('corpusDir', 'C:\\Users\\T.C.I\\.ir_datasets\\antique')\u001b[0m\n\u001b[1;37m                                                                                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "    def __init__(self, **args):\n",
    "        self.corpusDir = args.get('corpusDir', 'C:\\Users\\T.C.I\\.ir_datasets\\antique')\n",
    "        self.stopWordsFile = args.get('stopWordsFile', 'D:/IR_Project0/stop words.txt')\n",
    "        self.indexFile = args.get('indexFile', 'D:/IR_Project0/index.txt')\n",
    "        self.soundexIndexFile = args.get('soundexIndexFile', 'D:/IR_Project0/soundex-index.txt')\n",
    "        self.stopWords = []\n",
    "        self.index = defaultdict(list)\n",
    "        self.soundex_dic = defaultdict(set)\n",
    "        self.tfIndex = defaultdict(list)\n",
    "        self.dfIndex = defaultdict(int)\n",
    "        self.numOfDocuments = 0\n",
    "        self.porter_stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c4a4eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "    def extract_stop_words(self):\n",
    "        with open(self.stopWordsFile) as file:\n",
    "            self.stopWords = [line.strip().lower() for line in file if line.strip()]\n",
    "\n",
    "    def extract_terms(self, text):\n",
    "        text = self.shortcuts_dictionary.process_text(text)\n",
    "        text = re.sub(r'[^a-z0-9 ]', ' ', text)\n",
    "        dates = self.date_converter(text)\n",
    "        text = nltk.word_tokenize(text)\n",
    "        words = [word for word in text if word not in self.stopWords]\n",
    "        terms = []\n",
    "        for word in words:\n",
    "            term = self.porter_stemmer.stem(self.lemmatizer.lemmatize(word, 'v'))\n",
    "            terms.append(term)\n",
    "            if not re.search(r'\\W|\\d', term):\n",
    "                self.soundex_dic[self.linguistic_correction.get_soundex(term)].add(term)\n",
    "        for date in dates:\n",
    "            terms.append(date)\n",
    "        return terms\n",
    "\n",
    "    def word_to_num(self, word):\n",
    "        s = word.lower()[:3]\n",
    "        return self.monthDictionary[s]\n",
    "\n",
    "    def date_converter(self, line):\n",
    "        results = []\n",
    "        day = None\n",
    "        month = None\n",
    "        year = None\n",
    "        regex = re.search(r'([0]?\\d|[1][0-2])[/-]([0-3]?\\d)[/-]([1-2]\\d{3}|\\d{2})', line)\n",
    "        month_regex = re.search(\n",
    "            r'([0-3]?\\d)\\s*(Jan(?:uary)?(?:aury)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug('\n",
    "            '?:ust)?|Sept?(?:ember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?(?:emeber)?).?,?\\s([1-2]\\d{3})',\n",
    "            line)\n",
    "        rev_month_regex = re.search(\n",
    "            r'(Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug(?:ust)?|Sept?(?:ember)?|Oct('\n",
    "            '?:ober)?|Nov(?:ember)?|Dec(?:ember)?).?[-\\s]([0-3]?\\d)(?:st|nd|rd|th)?[-,\\s]\\s*([1-2]\\d{3})',\n",
    "            line)\n",
    "        no_day_regex = re.search(\n",
    "            r'(Jan(?:uary)?(?:aury)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug(?:ust)?|Sept?('\n",
    "            '?:ember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?(?:emeber)?).?,?[\\s]([1-2]\\d{3}|\\d{2})',\n",
    "            line)\n",
    "        no_day_digits_regex = re.search(r'([0]?\\d|[1][0-2])[/\\s]([1-2]\\d{3})', line)\n",
    "        year_only_regex = re.search(r'([1-2]\\d{3})', line)\n",
    "        if regex:\n",
    "            month = regex.group(1)\n",
    "            day = regex.group(2)\n",
    "            year = regex.group(3)\n",
    "        elif month_regex:\n",
    "            day = month_regex.group(1)\n",
    "            month = self.word_to_num(month_regex.group(2))\n",
    "            year = month_regex.group(3)\n",
    "        elif rev_month_regex:\n",
    "            day = rev_month_regex.group(2)\n",
    "            month = self.word_to_num(rev_month_regex.group(1))\n",
    "            year = rev_month_regex.group(3)\n",
    "        elif no_day_regex:\n",
    "            month = self.word_to_num(no_day_regex.group(1))\n",
    "            year = no_day_regex.group(2)\n",
    "        elif no_day_digits_regex:\n",
    "            month = no_day_digits_regex.group(1)\n",
    "            year = no_day_digits_regex.group(2)\n",
    "        elif year_only_regex:\n",
    "            year = year_only_regex.group(0)\n",
    "        if day or month or year:\n",
    "            year = year if year else '1900'\n",
    "            month = month.zfill(2) if month else '01'\n",
    "            day = day.zfill(2) if day else '01'\n",
    "            if day == '00':\n",
    "                day = '01'\n",
    "            if len(year) == 2:\n",
    "                year = '19' + year\n",
    "            results.append(year + month + day)\n",
    "        return results\n",
    "\n",
    "    def process_query(self, query):\n",
    "        terms = self.extract_terms(query)\n",
    "        print(terms)\n",
    "        if not len(terms):\n",
    "            return 'Not found'\n",
    "\n",
    "        union = set()\n",
    "        for term in terms:\n",
    "            if not self.index.get(term, None):\n",
    "                continue\n",
    "            term_documents = self.index[term]\n",
    "            documents_names = {term_document[0] for term_document in term_documents}\n",
    "            union |= documents_names\n",
    "\n",
    "        return self.rank_documents(terms, list(union))\n",
    "\n",
    "    def parse_corpus(self):\n",
    "        files_list = os.listdir(self.corpusDir)\n",
    "        for file in files_list:\n",
    "            yield self.parse_document(os.path.join(self.corpusDir, file))\n",
    "\n",
    "    def parse_document(self, document_file):\n",
    "        with open(document_file) as file:\n",
    "            lines = '\\n'.join(file.readlines())\n",
    "        document_name = Path(document_file).stem\n",
    "        return {'name': document_name, 'terms': self.extract_terms(lines)} if lines else {}\n",
    "\n",
    "    def create_index(self):\n",
    "        self.extract_stop_words()\n",
    "        for document in self.parse_corpus():\n",
    "            if document:\n",
    "                self.numOfDocuments += 1\n",
    "                document_name = document['name']\n",
    "                terms = document['terms']\n",
    "                document_index = defaultdict(lambda: [document_name, []])\n",
    "                for position, term in enumerate(terms):\n",
    "                    document_index[term][1].append(position)\n",
    "\n",
    "                norm = math.sqrt(\n",
    "                    sum([len(positions_vector) ** 2 for term, (document_name, positions_vector) in\n",
    "                         document_index.items()]))\n",
    "\n",
    "                for term, (document_name, positions_vector) in document_index.items():\n",
    "                    self.tfIndex[term].append('%.5f' % (len(positions_vector) / norm))\n",
    "                    self.dfIndex[term] += 1\n",
    "\n",
    "                for term, positions_vector in document_index.items():\n",
    "                    self.index[term].append(positions_vector)\n",
    "        self.write_index()\n",
    "\n",
    "    def write_index(self):\n",
    "        with open(self.indexFile, 'w') as file:\n",
    "            print(self.numOfDocuments, file=file)\n",
    "            for term in self.index.keys():\n",
    "                term_documents = []\n",
    "                for document_term_index in self.index[term]:\n",
    "                    document_name = document_term_index[0]\n",
    "                    positions_vector = document_term_index[1]\n",
    "                    term_documents.append(':'.join([str(document_name), ','.join(map(str, positions_vector))]))\n",
    "\n",
    "                term_documents_positions = ';'.join(term_documents)\n",
    "                documents_tf = ','.join(map(str, self.tfIndex[term]))\n",
    "                term_idf = '%.5f' % (self.numOfDocuments / self.dfIndex[term])\n",
    "                print('|'.join((term, term_documents_positions, documents_tf, term_idf)), file=file)\n",
    "\n",
    "        with open(self.soundexIndexFile, 'w') as file:\n",
    "            print('\\n'.join([code + ':' + ','.join(group) for code, group in self.soundex_dic.items()]), file=file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125f777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IndexCreator:\n",
    "    shortcuts_dictionary = ShortcutsDictionary()\n",
    "    linguistic_correction = LinguisticCorrection()\n",
    "    monthDictionary = dict(jan='01', feb='02', mar='03', apr='04', may='05', jun='06', jul='07', aug='08', sep='09',\n",
    "                           oct='10', nov='11', dec='12')\n",
    "\n",
    "    def __init__(self, **args):\n",
    "        self.corpusDir = args.get('corpusDir', 'D:/IR_Project/corpus')\n",
    "        self.stopWordsFile = args.get('stopWordsFile', 'D:/IR_Project/stop words.txt')\n",
    "        self.indexFile = args.get('indexFile', 'D:/IR_Project/index.txt')\n",
    "        self.soundexIndexFile = args.get('soundexIndexFile', 'D:/IR_Project/soundex-index.txt')\n",
    "        self.stopWords = []\n",
    "        self.index = defaultdict(list)\n",
    "        self.soundex_dic = defaultdict(set)\n",
    "        self.tfIndex = defaultdict(list)\n",
    "        self.dfIndex = defaultdict(int)\n",
    "        self.numOfDocuments = 0\n",
    "        self.porter_stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def extract_stop_words(self):\n",
    "        with open(self.stopWordsFile) as file:\n",
    "            self.stopWords = [line.strip().lower() for line in file if line.strip()]\n",
    "\n",
    "    def extract_terms(self, text):\n",
    "        text = self.shortcuts_dictionary.process_text(text)\n",
    "        text = re.sub(r'[^a-z0-9 ]', ' ', text)\n",
    "        dates = self.date_converter(text)\n",
    "        text = nltk.word_tokenize(text)\n",
    "        words = [word for word in text if word not in self.stopWords]\n",
    "        terms = []\n",
    "        for word in words:\n",
    "            term = self.porter_stemmer.stem(self.lemmatizer.lemmatize(word, 'v'))\n",
    "            terms.append(term)\n",
    "            if not re.search(r'\\W|\\d', term):\n",
    "                self.soundex_dic[self.linguistic_correction.get_soundex(term)].add(term)\n",
    "        for date in dates:\n",
    "            terms.append(date)\n",
    "        return terms\n",
    "\n",
    "    def word_to_num(self, word):\n",
    "        s = word.lower()[:3]\n",
    "        return self.monthDictionary[s]\n",
    "\n",
    "    def date_converter(self, line):\n",
    "        results = []\n",
    "        day = None\n",
    "        month = None\n",
    "        year = None\n",
    "        regex = re.search(r'([0]?\\d|[1][0-2])[/-]([0-3]?\\d)[/-]([1-2]\\d{3}|\\d{2})', line)\n",
    "        month_regex = re.search(\n",
    "            r'([0-3]?\\d)\\s*(Jan(?:uary)?(?:aury)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug('\n",
    "            '?:ust)?|Sept?(?:ember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?(?:emeber)?).?,?\\s([1-2]\\d{3})',\n",
    "            line)\n",
    "        rev_month_regex = re.search(\n",
    "            r'(Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug(?:ust)?|Sept?(?:ember)?|Oct('\n",
    "            '?:ober)?|Nov(?:ember)?|Dec(?:ember)?).?[-\\s]([0-3]?\\d)(?:st|nd|rd|th)?[-,\\s]\\s*([1-2]\\d{3})',\n",
    "            line)\n",
    "        no_day_regex = re.search(\n",
    "            r'(Jan(?:uary)?(?:aury)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug(?:ust)?|Sept?('\n",
    "            '?:ember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?(?:emeber)?).?,?[\\s]([1-2]\\d{3}|\\d{2})',\n",
    "            line)\n",
    "        no_day_digits_regex = re.search(r'([0]?\\d|[1][0-2])[/\\s]([1-2]\\d{3})', line)\n",
    "        year_only_regex = re.search(r'([1-2]\\d{3})', line)\n",
    "        if regex:\n",
    "            month = regex.group(1)\n",
    "            day = regex.group(2)\n",
    "            year = regex.group(3)\n",
    "        elif month_regex:\n",
    "            day = month_regex.group(1)\n",
    "            month = self.word_to_num(month_regex.group(2))\n",
    "            year = month_regex.group(3)\n",
    "        elif rev_month_regex:\n",
    "            day = rev_month_regex.group(2)\n",
    "            month = self.word_to_num(rev_month_regex.group(1))\n",
    "            year = rev_month_regex.group(3)\n",
    "        elif no_day_regex:\n",
    "            month = self.word_to_num(no_day_regex.group(1))\n",
    "            year = no_day_regex.group(2)\n",
    "        elif no_day_digits_regex:\n",
    "            month = no_day_digits_regex.group(1)\n",
    "            year = no_day_digits_regex.group(2)\n",
    "        elif year_only_regex:\n",
    "            year = year_only_regex.group(0)\n",
    "        if day or month or year:\n",
    "            year = year if year else '1900'\n",
    "            month = month.zfill(2) if month else '01'\n",
    "            day = day.zfill(2) if day else '01'\n",
    "            if day == '00':\n",
    "                day = '01'\n",
    "            if len(year) == 2:\n",
    "                year = '19' + year\n",
    "            results.append(year + month + day)\n",
    "        return results\n",
    "\n",
    "    def process_query(self, query):\n",
    "        terms = self.extract_terms(query)\n",
    "        print(terms)\n",
    "        if not len(terms):\n",
    "            return 'Not found'\n",
    "\n",
    "        union = set()\n",
    "        for term in terms:\n",
    "            if not self.index.get(term, None):\n",
    "                continue\n",
    "            term_documents = self.index[term]\n",
    "            documents_names = {term_document[0] for term_document in term_documents}\n",
    "            union |= documents_names\n",
    "\n",
    "        return self.rank_documents(terms, list(union))\n",
    "\n",
    "    def parse_corpus(self):\n",
    "        files_list = os.listdir(self.corpusDir)\n",
    "        for file in files_list:\n",
    "            yield self.parse_document(os.path.join(self.corpusDir, file))\n",
    "\n",
    "    def parse_document(self, document_file):\n",
    "        with open(document_file) as file:\n",
    "            lines = '\\n'.join(file.readlines())\n",
    "        document_name = Path(document_file).stem\n",
    "        return {'name': document_name, 'terms': self.extract_terms(lines)} if lines else {}\n",
    "\n",
    "    def create_index(self):\n",
    "        self.extract_stop_words()\n",
    "        for document in self.parse_corpus():\n",
    "            if document:\n",
    "                self.numOfDocuments += 1\n",
    "                document_name = document['name']\n",
    "                terms = document['terms']\n",
    "                document_index = defaultdict(lambda: [document_name, []])\n",
    "                for position, term in enumerate(terms):\n",
    "                    document_index[term][1].append(position)\n",
    "\n",
    "                norm = math.sqrt(\n",
    "                    sum([len(positions_vector) ** 2 for term, (document_name, positions_vector) in\n",
    "                         document_index.items()]))\n",
    "\n",
    "                for term, (document_name, positions_vector) in document_index.items():\n",
    "                    self.tfIndex[term].append('%.5f' % (len(positions_vector) / norm))\n",
    "                    self.dfIndex[term] += 1\n",
    "\n",
    "                for term, positions_vector in document_index.items():\n",
    "                    self.index[term].append(positions_vector)\n",
    "        self.write_index()\n",
    "\n",
    "    def write_index(self):\n",
    "        with open(self.indexFile, 'w') as file:\n",
    "            print(self.numOfDocuments, file=file)\n",
    "            for term in self.index.keys():\n",
    "                term_documents = []\n",
    "                for document_term_index in self.index[term]:\n",
    "                    document_name = document_term_index[0]\n",
    "                    positions_vector = document_term_index[1]\n",
    "                    term_documents.append(':'.join([str(document_name), ','.join(map(str, positions_vector))]))\n",
    "\n",
    "                term_documents_positions = ';'.join(term_documents)\n",
    "                documents_tf = ','.join(map(str, self.tfIndex[term]))\n",
    "                term_idf = '%.5f' % (self.numOfDocuments / self.dfIndex[term])\n",
    "                print('|'.join((term, term_documents_positions, documents_tf, term_idf)), file=file)\n",
    "\n",
    "        with open(self.soundexIndexFile, 'w') as file:\n",
    "            print('\\n'.join([code + ':' + ','.join(group) for code, group in self.soundex_dic.items()]), file=file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a223f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # corpus_dir = input('Enter corpus directory path')\n",
    "    # stop_words_file = input('Enter stop words file path')\n",
    "    # index_file = input('Enter index file output path')\n",
    "    IndexCreator().create_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25140a1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IndexCreator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mIndexCreator\u001b[49m()\u001b[38;5;241m.\u001b[39mcreate_index()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'IndexCreator' is not defined"
     ]
    }
   ],
   "source": [
    "IndexCreator().create_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2314587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bceac5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07db7ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e360bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600a7a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d422c342",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74e339a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4d9ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13dacd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c39a73a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e690ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2210cfc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db19b80f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9105df09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b0d71a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfcc46e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916ffa35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a38ec25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af79ae99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff885f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6a0371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb8f685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3030bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe77e803",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae9d998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fef4655",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a403f320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574ddd72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca3c96a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9791b8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e94454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8bac97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ac3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b56ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806c67a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a8f8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1c37c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a44de4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0ef988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd738a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
