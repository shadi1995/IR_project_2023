{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29f093a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc2a8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82489c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50863cec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5298da5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b691fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4bcf74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c33913",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba30f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import functools\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from linguistic_correcter import LinguisticCorrection\n",
    "from shortcuts_dictionary import ShortcutsDictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22970baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexQuery:\n",
    "    shortcuts_dictionary = ShortcutsDictionary()\n",
    "    linguistic_correction = LinguisticCorrection()\n",
    "    monthDictionary = dict(jan='01', feb='02', mar='03', apr='04', may='05', jun='06', jul='07', aug='08', sep='09',\n",
    "                           oct='10', nov='11', dec='12')\n",
    "\n",
    "    def __init__(self, **args):\n",
    "        self.stopWordsFile = args.get('stopWordsFile', 'D:/IR_Project/stop words.txt')\n",
    "        self.indexFile = args.get('indexFile', 'D:/IR_Project/index.txt')\n",
    "        self.soundexIndexFile = args.get('soundexIndexFile', 'D:/IR_Project/soundex-index.txt')\n",
    "        self.stopWords = []\n",
    "        self.index = defaultdict(list)\n",
    "        self.soundex_dic = defaultdict(list)\n",
    "        self.tfIndex = defaultdict(list)\n",
    "        self.idfIndex = defaultdict(float)\n",
    "        self.numOfDocuments = 0\n",
    "        self.porter_stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.extract_stop_words()\n",
    "        self.read_index()\n",
    "        self.read_soundex_index()\n",
    "\n",
    "    def extract_stop_words(self):\n",
    "        with open(self.stopWordsFile) as file:\n",
    "            self.stopWords = [line.strip().lower() for line in file if line.strip()]\n",
    "\n",
    "    def extract_terms(self, text):\n",
    "        text = self.shortcuts_dictionary.process_text(text)\n",
    "        text = re.sub(r'[^a-z0-9 ]', ' ', text)\n",
    "        dates = self.date_converter(text)\n",
    "        text = nltk.word_tokenize(text)\n",
    "        words = [word for word in text if word not in self.stopWords]\n",
    "        terms = [self.porter_stemmer.stem(self.lemmatizer.lemmatize(word, 'v')) for word in words]\n",
    "        for date in dates:\n",
    "            terms.append(date)\n",
    "        return terms\n",
    "\n",
    "    def date_converter(self, line):\n",
    "        results = []\n",
    "        day = None\n",
    "        month = None\n",
    "        year = None\n",
    "        regex = re.search(r'([0]?\\d|[1][0-2])[/-]([0-3]?\\d)[/-]([1-2]\\d{3}|\\d{2})', line)\n",
    "        month_regex = re.search(\n",
    "            r'([0-3]?\\d)\\s*(Jan(?:uary)?(?:aury)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug('\n",
    "            '?:ust)?|Sept?(?:ember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?(?:emeber)?).?,?\\s([1-2]\\d{3})',\n",
    "            line)\n",
    "        rev_month_regex = re.search(\n",
    "            r'(Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug(?:ust)?|Sept?(?:ember)?|Oct('\n",
    "            '?:ober)?|Nov(?:ember)?|Dec(?:ember)?).?[-\\s]([0-3]?\\d)(?:st|nd|rd|th)?[-,\\s]\\s*([1-2]\\d{3})',\n",
    "            line)\n",
    "        no_day_regex = re.search(\n",
    "            r'(Jan(?:uary)?(?:aury)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|June?|July?|Aug(?:ust)?|Sept?('\n",
    "            '?:ember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?(?:emeber)?).?,?[\\s]([1-2]\\d{3}|\\d{2})',\n",
    "            line)\n",
    "        no_day_digits_regex = re.search(r'([0]?\\d|[1][0-2])[/\\s]([1-2]\\d{3})', line)\n",
    "        year_only_regex = re.search(r'([1-2]\\d{3})', line)\n",
    "        if regex:\n",
    "            month = regex.group(1)\n",
    "            day = regex.group(2)\n",
    "            year = regex.group(3)\n",
    "        elif month_regex:\n",
    "            day = month_regex.group(1)\n",
    "            month = self.word_to_num(month_regex.group(2))\n",
    "            year = month_regex.group(3)\n",
    "        elif rev_month_regex:\n",
    "            day = rev_month_regex.group(2)\n",
    "            month = self.word_to_num(rev_month_regex.group(1))\n",
    "            year = rev_month_regex.group(3)\n",
    "        elif no_day_regex:\n",
    "            month = self.word_to_num(no_day_regex.group(1))\n",
    "            year = no_day_regex.group(2)\n",
    "        elif no_day_digits_regex:\n",
    "            month = no_day_digits_regex.group(1)\n",
    "            year = no_day_digits_regex.group(2)\n",
    "        elif year_only_regex:\n",
    "            year = year_only_regex.group(0)\n",
    "        if day or month or year:\n",
    "            year = year if year else '1900'\n",
    "            month = month.zfill(2) if month else '01'\n",
    "            day = day.zfill(2) if day else '01'\n",
    "            if day == '00':\n",
    "                day = '01'\n",
    "            if len(year) == 2:\n",
    "                year = '19' + year\n",
    "            results.append(year + month + day)\n",
    "        return results\n",
    "\n",
    "    def word_to_num(self, word):\n",
    "        s = word.lower()[:3]\n",
    "        return self.monthDictionary[s]\n",
    "\n",
    "    def read_index(self):\n",
    "        with open(self.indexFile) as file:\n",
    "            self.numOfDocuments = float(file.readline().strip())\n",
    "            for line in file:\n",
    "                line = line.rstrip()\n",
    "                term, term_documents, documents_tf, term_idf = line.split('|')\n",
    "                term_documents = term_documents.split(';')\n",
    "                term_documents = [x.split(':') for x in term_documents]\n",
    "                term_documents = [[x[0], map(int, x[1].split(','))] for x in term_documents]\n",
    "                self.index[term] = term_documents\n",
    "                documents_tf = documents_tf.split(',')\n",
    "                self.tfIndex[term] = [float(tf) for tf in documents_tf]\n",
    "                self.idfIndex[term] = float(term_idf)\n",
    "\n",
    "    def read_soundex_index(self):\n",
    "        with open(self.soundexIndexFile) as file:\n",
    "            for line in file:\n",
    "                if not line:\n",
    "                    continue\n",
    "                split = line.strip().split(':')\n",
    "                self.soundex_dic[split[0]] = split[1].split(',')\n",
    "\n",
    "    def rank_documents(self, terms, documents):\n",
    "        vector_space = defaultdict(lambda: [0] * len(terms))\n",
    "        query_vector = [0] * len(terms)\n",
    "\n",
    "        for term_index, term in enumerate(terms):\n",
    "            if term not in self.index:\n",
    "                continue\n",
    "\n",
    "            query_vector[term_index] = self.idfIndex[term]\n",
    "\n",
    "            for docIndex, (document_name, positions_vector) in enumerate(self.index[term]):\n",
    "                if document_name in documents:\n",
    "                    vector_space[document_name][term_index] = self.tfIndex[term][docIndex]\n",
    "\n",
    "        doc_scores = [[document_name,\n",
    "                       self.dot_product(document_vector, query_vector)]\n",
    "                      for document_name, document_vector in vector_space.items()]\n",
    "\n",
    "        doc_scores.sort(reverse=True, key=itemgetter(1))\n",
    "\n",
    "        return [document_name[0] for document_name in doc_scores]\n",
    "\n",
    "    @staticmethod\n",
    "    def dot_product(vec1, vec2):\n",
    "        if len(vec1) != len(vec2):\n",
    "            return 0\n",
    "        return sum([x * y for x, y in zip(vec1, vec2)])\n",
    "\n",
    "    def try_term_correction(self, term):\n",
    "        term_code = self.linguistic_correction.get_soundex(term)\n",
    "        group = self.soundex_dic.get(term_code, None)\n",
    "        if not group:\n",
    "            return None\n",
    "        return self.linguistic_correction.best_similarity(term, group)\n",
    "\n",
    "    def process_any_query(self, query):\n",
    "        terms = self.extract_terms(query)\n",
    "        if not len(terms):\n",
    "            return 'Not found'\n",
    "\n",
    "        union = set()\n",
    "        for term in terms:\n",
    "            if not self.index.get(term, None):\n",
    "                term_correction = self.try_term_correction(term)\n",
    "                if not term_correction or term_correction[1] > 3:\n",
    "                    continue\n",
    "                term = term_correction[0]\n",
    "            term_documents = self.index[term]\n",
    "            documents_names = {term_document[0] for term_document in term_documents}\n",
    "            union |= documents_names\n",
    "\n",
    "        return self.rank_documents(terms, list(union))\n",
    "\n",
    "    def process_quote_query(self, query):\n",
    "        terms = self.extract_terms(query)\n",
    "        if not len(terms):\n",
    "            return 'Not found'\n",
    "\n",
    "        for index, term in enumerate(terms):\n",
    "            if term not in self.index:\n",
    "                term_correction = self.try_term_correction(term)\n",
    "                if not term_correction or term_correction[1] > 3:\n",
    "                    # if a term doesn't appear in the index\n",
    "                    # there can't be any document matching it\n",
    "                    return []\n",
    "                terms[index] = term_correction[0]\n",
    "\n",
    "        docs_vector = [self.index[term] for term in terms]\n",
    "        docs = [[x[0] for x in p] for p in docs_vector]\n",
    "\n",
    "        docs = self.intersect_lists(docs)\n",
    "\n",
    "        for i in range(len(docs_vector)):\n",
    "            docs_vector[i] = [x for x in docs_vector[i] if x[0] in docs]\n",
    "\n",
    "        docs_vector = copy.deepcopy(docs_vector)\n",
    "\n",
    "        for i in range(len(docs_vector)):\n",
    "            for j in range(len(docs_vector[i])):\n",
    "                docs_vector[i][j][1] = [x - i for x in docs_vector[i][j][1]]\n",
    "\n",
    "        result = []\n",
    "        for i in range(len(docs_vector[0])):\n",
    "            li = self.intersect_lists([x[i][1] for x in docs_vector])\n",
    "            if not li:\n",
    "                continue\n",
    "            else:\n",
    "                result.append(docs_vector[0][i][0])\n",
    "\n",
    "        return self.rank_documents(terms, result)\n",
    "\n",
    "    @staticmethod\n",
    "    def intersect_lists(lists):\n",
    "        if len(lists) == 0:\n",
    "            return []\n",
    "        # start intersecting from the smaller list\n",
    "        lists.sort(key=len)\n",
    "        return list(functools.reduce(lambda x, y: set(x) & set(y), lists))\n",
    "\n",
    "    def query(self, search_query):\n",
    "        return self.process_quote_query(search_query) if search_query.startswith('\"') \\\n",
    "            else self.process_any_query(search_query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ed97e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_query = IndexQuery()\n",
    "while True:\n",
    "        search_query = input('Enter something to find:\\n')\n",
    "\n",
    "        if search_query == 'exit':\n",
    "            break\n",
    "\n",
    "        result = index_query.query(search_query)\n",
    "        print(f'Total number of results: {len(result)} document')\n",
    "        print('\\n'.join(result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45405451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
