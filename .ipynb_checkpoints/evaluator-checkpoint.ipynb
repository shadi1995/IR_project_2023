{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b9e5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "from index_query import IndexQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6a9754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Evaluator:\n",
    "    indexQuery = IndexQuery()\n",
    "    START_PATTERN = r'^\\*FIND.*\\d+$'\n",
    "    END_PATTERN = r'^\\*STOP$'\n",
    "\n",
    "    def __init__(self, **kvargs):\n",
    "        self.queriesFile = kvargs.get('queriesFile', \"D:/IR_Project/Queries.txt\")\n",
    "        self.relevanceFile = kvargs.get('relevanceFile', \"D:/IR_Project/relevance.txt\")\n",
    "        self.queryStatistics = defaultdict(lambda: defaultdict(float))\n",
    "        self.queryRelevantDocuments = defaultdict(list)\n",
    "        self.average_precision_scores = defaultdict(float)\n",
    "        self.f_scores = defaultdict(float)\n",
    "\n",
    "    def read_relevant_documents(self):\n",
    "        with open(self.relevanceFile) as file:\n",
    "            for line in file:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                line = line.split()\n",
    "                self.queryRelevantDocuments[line[0]] = line[1:]\n",
    "\n",
    "    def calc_precision(self, retrieved_documents, relevant_documents_retrieved):\n",
    "        total_relevant_documents_retrieved = len(relevant_documents_retrieved)\n",
    "        total_documents_retrieved = len(retrieved_documents)\n",
    "        return total_relevant_documents_retrieved / total_documents_retrieved\n",
    "\n",
    "    def calc_recall(self, query_number, relevant_documents_retrieved):\n",
    "        total_relevant_documents_retrieved = len(relevant_documents_retrieved)\n",
    "        total_relevant_documents = len(self.queryRelevantDocuments[query_number])\n",
    "        return total_relevant_documents_retrieved / total_relevant_documents\n",
    "\n",
    "    def get_relevant_documents_of(self, query_number, retrieved_documents):\n",
    "        return list(set(self.queryRelevantDocuments[query_number]) & set(retrieved_documents))\n",
    "\n",
    "    def calc_query_statistics(self, query_number, retrieved_documents):\n",
    "        relevant_documents_retrieved = self.get_relevant_documents_of(query_number, retrieved_documents)\n",
    "        precision = self.calc_precision(retrieved_documents, relevant_documents_retrieved)\n",
    "        recall = self.calc_recall(query_number, relevant_documents_retrieved)\n",
    "        f_score = (2 * precision * recall) / (precision + recall) if (precision + recall) != 0 else 0\n",
    "        p_k = []\n",
    "        for rank in range(len(retrieved_documents)):\n",
    "            k_retrieved_documents = retrieved_documents[:rank + 1]\n",
    "            k_relevant_documents_retrieved = self.get_relevant_documents_of(query_number, k_retrieved_documents)\n",
    "            p_k.append(self.calc_precision(k_retrieved_documents, k_relevant_documents_retrieved))\n",
    "\n",
    "        avg_precision = sum(p_k) / len(relevant_documents_retrieved) if len(relevant_documents_retrieved) != 0 else 0\n",
    "\n",
    "        self.queryStatistics[query_number]['precision'] = precision\n",
    "        self.queryStatistics[query_number]['recall'] = recall\n",
    "        self.f_scores[query_number] = f_score\n",
    "        self.average_precision_scores[query_number] = avg_precision\n",
    "\n",
    "    def calc_mean_average_precision(self):\n",
    "        return sum(self.average_precision_scores.values()) / len(self.average_precision_scores)\n",
    "\n",
    "    def calc_mean_f_score(self):\n",
    "        return sum(self.f_scores.values()) / len(self.f_scores)\n",
    "\n",
    "    def pass_queries(self):\n",
    "        with open(self.queriesFile) as queries_file:\n",
    "            match = False\n",
    "            lines = []\n",
    "            query_number = None\n",
    "\n",
    "            for line in queries_file:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                if re.match(self.END_PATTERN, line):\n",
    "                    query = ' '.join(lines)\n",
    "                    retrieved_documents = self.indexQuery.query(query)\n",
    "                    self.calc_query_statistics(query_number, retrieved_documents)\n",
    "                    break\n",
    "                elif re.match(self.START_PATTERN, line):\n",
    "                    if not match:\n",
    "                        match = True\n",
    "                        query_number = re.findall(r'\\d+', line)[0]\n",
    "                        continue\n",
    "                    query = ' '.join(lines)\n",
    "                    retrieved_documents = self.indexQuery.query(query)\n",
    "                    self.calc_query_statistics(query_number, retrieved_documents)\n",
    "                    query_number = re.findall(r'\\d+', line)[0]\n",
    "                elif match:\n",
    "                    lines.append(line)\n",
    "\n",
    "    def get_average_precision_scores(self):\n",
    "        return self.average_precision_scores\n",
    "\n",
    "    def get_query_statistics(self):\n",
    "        return self.queryStatistics\n",
    "\n",
    "    def get_f_scores(self):\n",
    "        return self.f_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c33c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    evaluator = Evaluator()\n",
    "    evaluator.read_relevant_documents()\n",
    "    evaluator.pass_queries()\n",
    "    query_statistics = evaluator.get_query_statistics()\n",
    "    precision_scores = evaluator.get_average_precision_scores()\n",
    "    f_scores = evaluator.get_f_scores()\n",
    "    print(f\"Mean average precision = {evaluator.calc_mean_average_precision():.5f}\")\n",
    "    print(f\"Average F score = {evaluator.calc_mean_f_score():.5f}\")\n",
    "    for query_number, statistics in query_statistics.items():\n",
    "        print(f\"\"\"Query #{query_number}\n",
    "         - precision = {statistics['precision']:.5f}\n",
    "         - recall = {statistics['recall']:.5f}\n",
    "         - F score = {f_scores[query_number]:.5f}\n",
    "         - Average precision score = {precision_scores[query_number]:.5f}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a18d29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fb5372",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
